{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aeddea7",
   "metadata": {},
   "source": [
    "# A General Perspective on Graph Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be0b8b6",
   "metadata": {},
   "source": [
    "## General GNN Framework\n",
    "\n",
    "Consider the following GNN layer:\n",
    "\n",
    "$$\n",
    "    h_v^{(l)} = \\sigma \\left( \\operatorname{AGG}^{(l)} \\left( \\{\\operatorname{MSG}^{(l)}(h_u^{(l-1)} \\mid u \\in N(v))\\} \\right) \\right)\n",
    "$$\n",
    "\n",
    "1. Explain the role of `AGG` and `MSG` components.\n",
    "1. Why must `AGG` be permutation invariant?\n",
    "1. What issue arises in this formulation? (Hint: Consider how node $v$ incorporates information about itself.)\n",
    "1. How can we modify this equation to fix the issue in (3)?\n",
    "1. Modify the layer so that self-information and neighbor-information are treated differently. Provide the updated equation. (Hint: You are not limited to a single trainable weight matrix.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f541fe",
   "metadata": {},
   "source": [
    "## Depth of a GNN\n",
    "\n",
    "Consider the following GNN layer:\n",
    "$$\n",
    "    h_v^{(l)} = \\operatorname{CONCAT} \\big( \\operatorname{AGG} \\big( \\{ m_u^{(l)} \\mid u \\in N(v) \\} \\big), m_v^{(l)} \\big)\n",
    "$$\n",
    "where $m_u^{(l)}=\\operatorname{MSG}^{(l)}(h_u^{(l-1)})$ are the messages from neighbors and $m_v^{(l)}=\\operatorname{MSG}^{(l)}(h_v^{(l-1)})$ is the self-message.\n",
    "\n",
    "1. Assume that the `AGG` is the `SUM` function, and the messages $m_u^{(l)}$ and $m_v^{(l)}$ are $d$-dimensional vectors. What is the dimension of $h_v^{(l)}$?\n",
    "1. Discuss potential issues or challenges this concatenation might introduce when stacking multiple GNN layers.\n",
    "1. Suggest a technique to control the dimension after concatenation.\n",
    "1. Now, assume that we change the layer definition so that the concatenated vector is fed to a $k$-layer multi-layer perceptron (MLP) as given in the following:\n",
    "    $$\n",
    "        h_v^{(l)} = \\operatorname{MLP} \\big( \\operatorname{CONCAT} \\big( \\operatorname{AGG} \\big( \\{ m_u^{(l)} \\mid u \\in N(v) \\} \\big), m_v^{(l)} \\big) \\big)\n",
    "    $$\n",
    "    If we stack $L$ such layers, what will be the total depth of the GNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1207d4",
   "metadata": {},
   "source": [
    "## Graph Convolutional Networks\n",
    "\n",
    "Consider the layer definition of the GCN by Kipf & Welling (paper [link](https://arxiv.org/abs/1609.02907)):\n",
    "\n",
    "$$\n",
    "    h_v^{(l+1)} = \\sigma\\left( \\sum_{u \\in N(v) \\cup \\{v\\}} \\frac{1}{\\sqrt{\\tilde{d}_v \\tilde{d}_u}} \\, h_u^{(l)} W^{(l)} \\right)\n",
    "$$\n",
    "where $\\tilde{d}_v$ is the degree of node $v$ in the augmented adjacency $\\tilde{A} = A + I$ after adding self-loops.\n",
    "\n",
    "1. Compared to the simple normalization factor $\\frac{1}{|N(v)|}$ we have seen in the lecture (W4, slide 17), what could be the reason to have $\\frac{1}{\\sqrt{\\tilde{d}_v \\tilde{d}_u}}$ (symmetric normalization)?\n",
    "\n",
    "1. How does the GCN process a node's own message differently from its neighbors' messages?\n",
    "\n",
    "1. GraphSAGE (SAmple and aggreGatE) by Hamilton et. al. (paper [link](https://arxiv.org/abs/1706.02216)) is an extension of the GCN framework. Given the pseudocode below, list three aspects of GraphSAGE that is different from the original GCN and discuss how these changes improve the method.\n",
    "\n",
    "    ![alt text](assets/w4_graphsage.png \"GraphSAGE Algorithm\")\n",
    "\n",
    "1. How does the Graph Attention Network by Veličković et. al. ([paper](https://arxiv.org/abs/1710.10903)) improve the GCN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581d3e9c",
   "metadata": {},
   "source": [
    "## Programming: Simple GNN with Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf46d0d",
   "metadata": {},
   "source": [
    "In this exercise, you'll implement a simple GNN with `pytorch` and test on the `Cora` dataset ([link](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.Planetoid.html)).\n",
    "\n",
    "The layer definition of the GNN you'll implement is given as follows:\n",
    "\n",
    "$$\n",
    "    h_v^{(l)} = \\sigma \\left( W_{\\text{self}}^{(l)} \\cdot h_v^{(l-1)} + W_{\\text{neigh}}^{(l)} \\cdot \\operatorname{AGG}^{(l)}\\left(\\{h_u^{(l-1)} \\mid u \\in N(v)\\}\\right) \\right)\n",
    "$$\n",
    "\n",
    "Basically, it uses two learnable weight matrices $W_{\\text{self}}$ and $W_{\\text{neigh}}$ that are multiplied with the messages from the node's self and aggregated messages from neighbors, respectively. Then, the resulting vectors are summed and fed to $\\sigma$.\n",
    "\n",
    "1. Complete the following code snippet given the following:\n",
    "    - Use `MEAN` aggretagor for neighbor messages.\n",
    "    - Use ReLU as $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c86c3f0",
   "metadata": {
    "tags": [
     "q_code"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ============================================================================\n",
    "# GRAPH NEURAL NETWORK (GNN) IMPLEMENTATION\n",
    "# ============================================================================\n",
    "# This code implements a simple Graph Neural Network that learns from graph-structured data.\n",
    "# \n",
    "# EXAMPLE GRAPH:\n",
    "#   Node 0 --- Node 1\n",
    "#     |          |\n",
    "#   Node 2 --- Node 3\n",
    "# \n",
    "# In this example:\n",
    "# - 4 nodes (0, 1, 2, 3)\n",
    "# - Edges: (0,1), (0,2), (1,3), (2,3)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class SimpleGNNLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Graph Neural Network layer that updates node features by:\n",
    "    1. Aggregating information from neighboring nodes (mean aggregation)\n",
    "    2. Combining own features with aggregated neighbor features\n",
    "    \n",
    "    INTUITION: Each node learns a new representation by looking at its \n",
    "    neighbors' features, similar to how you might update your opinion by \n",
    "    listening to your friends.\n",
    "    \n",
    "    EXAMPLE:\n",
    "    If Node 0 is connected to Nodes 1 and 2:\n",
    "    - Node 0's new features = f(Node 0's current features, average of Node 1 and Node 2's features)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        \"\"\"\n",
    "        Initialize the GNN layer with separate weight matrices.\n",
    "        \n",
    "        Args:\n",
    "            in_dim: Input feature dimension (e.g., 16 features per node)\n",
    "            out_dim: Output feature dimension (e.g., 32 features per node)\n",
    "        \n",
    "        EXAMPLE:\n",
    "        If in_dim=3 and out_dim=5:\n",
    "        - Each node starts with 3 features\n",
    "        - After this layer, each node will have 5 features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Separate linear transformations for self and neighbor features\n",
    "        # This allows the model to learn different importance for:\n",
    "        # - What I already know (self)\n",
    "        # - What my neighbors tell me (neighbors)\n",
    "        self.linear_self = nn.Linear(in_dim, out_dim, bias=True)\n",
    "        self.linear_neigh = nn.Linear(in_dim, out_dim, bias=True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Forward pass: Update all node features using graph structure.\n",
    "        \n",
    "        Args:\n",
    "            x: Node feature matrix, shape [N, in_dim]\n",
    "               N = number of nodes, in_dim = features per node\n",
    "               EXAMPLE: If we have 4 nodes with 3 features each:\n",
    "                        x = [[0.1, 0.2, 0.3],  # Node 0\n",
    "                             [0.4, 0.5, 0.6],  # Node 1\n",
    "                             [0.7, 0.8, 0.9],  # Node 2\n",
    "                             [1.0, 1.1, 1.2]]  # Node 3\n",
    "            \n",
    "            edge_index: Graph connectivity, shape [2, E]\n",
    "                        E = number of edges\n",
    "                        Format: [[target_nodes], [source_nodes]]\n",
    "                        EXAMPLE: For edges (1→0), (2→0), (3→1), (3→2):\n",
    "                                 edge_index = [[0, 0, 1, 2],  # target nodes (who receives)\n",
    "                                               [1, 2, 3, 3]]  # source nodes (who sends)\n",
    "        \n",
    "        Returns:\n",
    "            Updated node features, shape [N, out_dim]\n",
    "        \"\"\"\n",
    "        \n",
    "        # ====================================================================\n",
    "        # STEP 1: Extract edge information\n",
    "        # ====================================================================\n",
    "        # edges j -> i (col = source who sends, row = target who receives)\n",
    "        row, col = edge_index\n",
    "        # EXAMPLE: If edge_index = [[0, 1], [2, 3]]\n",
    "        #          row = [0, 1] (receivers)\n",
    "        #          col = [2, 3] (senders)\n",
    "        #          This means: Node 2 → Node 0, Node 3 → Node 1\n",
    "\n",
    "        # ====================================================================\n",
    "        # STEP 2: Gather messages from neighbors\n",
    "        # ====================================================================\n",
    "        # For each edge, get the source node's features\n",
    "        messages = x[col]\n",
    "        # EXAMPLE: If col = [1, 2, 2] and x has 4 nodes:\n",
    "        #          messages = [x[1], x[2], x[2]]\n",
    "        #          These are the features being \"sent\" along edges\n",
    "\n",
    "        # ====================================================================\n",
    "        # STEP 3: Aggregate messages by mean (average neighbor features)\n",
    "        # ====================================================================\n",
    "        # We want to compute the average features of neighbors for each node\n",
    "        \n",
    "        N = x.size(0)  # Number of nodes in the graph\n",
    "        # EXAMPLE: If x.shape = [4, 16], then N = 4\n",
    "        \n",
    "        # Initialize aggregation buffer with zeros\n",
    "        agg = torch.zeros(N, x.size(1), device=x.device)\n",
    "        # EXAMPLE: If N=4 nodes with 16 features: agg.shape = [4, 16]\n",
    "        \n",
    "        # Sum all incoming messages for each target node\n",
    "        # index_add_ accumulates values: agg[row[i]] += messages[i]\n",
    "        agg.index_add_(0, row, messages)\n",
    "        # EXAMPLE: If row = [0, 0, 1] and messages has 3 feature vectors:\n",
    "        #          agg[0] = messages[0] + messages[1]  (Node 0 receives 2 messages)\n",
    "        #          agg[1] = messages[2]                (Node 1 receives 1 message)\n",
    "        # \n",
    "        # It basically does:\n",
    "        # for k in range(len(index)):\n",
    "        #     target[index[k]] += source[k]\n",
    "        # This is done in parallel and very fast on GPU!\n",
    "        \n",
    "        # Count how many neighbors each node has (degree)\n",
    "        deg = torch.bincount(row, minlength=N).clamp(min=1).unsqueeze(1)\n",
    "        # EXAMPLE: If row = [0, 0, 1]:\n",
    "        #          bincount gives [2, 1, 0, 0] (Node 0 has 2 neighbors, Node 1 has 1)\n",
    "        #          clamp(min=1) gives [2, 1, 1, 1] (avoid division by zero)\n",
    "        #          unsqueeze(1) reshapes to [4, 1] for broadcasting\n",
    "        #          unsqueeze adds a dimension of size 1. \n",
    "        #          It changes the shape of the tensor.\n",
    "        \n",
    "        # Divide sum by count to get mean\n",
    "        agg = agg / deg\n",
    "        # EXAMPLE: If agg[0] = [2.0, 4.0] and deg[0] = 2:\n",
    "        #          agg[0] = [1.0, 2.0] (mean of two neighbors)\n",
    "\n",
    "        # ====================================================================\n",
    "        # STEP 4: Combine self features and aggregated neighbor features\n",
    "        # ====================================================================\n",
    "        # Update rule: new_features = W_self * self_features + W_neigh * neighbor_features\n",
    "        # This is like saying: \"My new representation is a weighted combination of \n",
    "        # what I know and what my neighbors tell me\"\n",
    "        \n",
    "        out = self.linear_self(x) + self.linear_neigh(agg)\n",
    "        # EXAMPLE: If x[0] = [1, 2, 3] and agg[0] = [4, 5, 6]:\n",
    "        #          out[0] = W_self @ [1,2,3] + W_neigh @ [4,5,6]\n",
    "        #          The model learns W_self and W_neigh during training\n",
    "\n",
    "        # ====================================================================\n",
    "        # STEP 5: Return updated node features\n",
    "        # ====================================================================\n",
    "        return out\n",
    "        # EXAMPLE OUTPUT: If input was [4, 16] and out_dim=32:\n",
    "        #                 output will be [4, 32] (same number of nodes, new features)\n",
    "\n",
    "\n",
    "class SimpleGNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A complete 2-layer Graph Neural Network model.\n",
    "    \n",
    "    ARCHITECTURE:\n",
    "    Input features → GNN Layer 1 → ReLU → GNN Layer 2 → Output features\n",
    "    \n",
    "    INTUITION: Stacking multiple GNN layers allows each node to gather \n",
    "    information from farther away in the graph:\n",
    "    - Layer 1: Each node sees its immediate neighbors\n",
    "    - Layer 2: Each node sees neighbors of neighbors (2-hop neighborhood)\n",
    "    \n",
    "    EXAMPLE USE CASE - Node Classification:\n",
    "    Input: Social network where each person has initial features\n",
    "    Output: Predict categories for each person (e.g., interests, communities)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        \"\"\"\n",
    "        Initialize the 2-layer GNN.\n",
    "        \n",
    "        Args:\n",
    "            in_dim: Input feature dimension\n",
    "            hidden_dim: Hidden layer dimension\n",
    "            out_dim: Output feature dimension (e.g., number of classes)\n",
    "        \n",
    "        EXAMPLE:\n",
    "        SimpleGNN(in_dim=10, hidden_dim=64, out_dim=7)\n",
    "        - Input: 10 features per node\n",
    "        - Hidden: 64 features after first layer\n",
    "        - Output: 7 features (e.g., 7 class predictions)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layer1 = SimpleGNNLayer(in_dim, hidden_dim)\n",
    "        self.layer2 = SimpleGNNLayer(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Forward pass through the entire GNN.\n",
    "        \n",
    "        Args:\n",
    "            data: Graph data object containing:\n",
    "                  - data.x: Node features [N, in_dim]\n",
    "                  - data.edge_index: Graph structure [2, E]\n",
    "        \n",
    "        Returns:\n",
    "            Node predictions/embeddings [N, out_dim]\n",
    "        \n",
    "        EXAMPLE:\n",
    "        Input:  4 nodes, each with 10 features\n",
    "        Output: 4 nodes, each with 7 class scores\n",
    "        \"\"\"\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # ====================================================================\n",
    "        # Apply GNN layers with ReLU non-linearity\n",
    "        # ====================================================================\n",
    "        \n",
    "        # First GNN layer: Aggregate 1-hop neighborhood information\n",
    "        x = self.layer1(x, edge_index)\n",
    "        # EXAMPLE: x.shape changes from [4, 10] to [4, 64]\n",
    "        \n",
    "        # Non-linearity: ReLU activation introduces non-linear transformations\n",
    "        # ReLU(x) = max(0, x) - sets negative values to 0\n",
    "        x = F.relu(x)\n",
    "        # EXAMPLE: If x[0] = [-1.0, 2.0, -0.5], then relu(x[0]) = [0.0, 2.0, 0.0]\n",
    "        \n",
    "        # Second GNN layer: Aggregate 2-hop neighborhood information\n",
    "        x = self.layer2(x, edge_index)\n",
    "        # EXAMPLE: x.shape changes from [4, 64] to [4, 7]\n",
    "        # Now each node has information from its 2-hop neighborhood\n",
    "\n",
    "        return x\n",
    "        # EXAMPLE OUTPUT: If predicting node categories:\n",
    "        #                 [[0.1, 0.8, 0.05, 0.02, 0.01, 0.01, 0.01],  # Node 0: likely class 1\n",
    "        #                  [0.7, 0.1, 0.1, 0.05, 0.02, 0.02, 0.01],   # Node 1: likely class 0\n",
    "        #                  ...]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# USAGE EXAMPLE\n",
    "# ============================================================================\n",
    "# \n",
    "# # Create a simple graph with 4 nodes\n",
    "# x = torch.randn(4, 10)  # 4 nodes, 10 features each\n",
    "# edge_index = torch.tensor([[0, 1, 2, 3],  # targets\n",
    "#                            [1, 0, 3, 2]], dtype=torch.long)  # sources\n",
    "# \n",
    "# # Create data object (using PyTorch Geometric format)\n",
    "# from torch_geometric.data import Data\n",
    "# data = Data(x=x, edge_index=edge_index)\n",
    "# \n",
    "# # Initialize and run the model\n",
    "# model = SimpleGNN(in_dim=10, hidden_dim=64, out_dim=7)\n",
    "# output = model(data)  # Shape: [4, 7] - 7 scores for each of 4 nodes\n",
    "# \n",
    "# # For node classification, apply softmax to get probabilities\n",
    "# predictions = torch.softmax(output, dim=1)\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baaab74",
   "metadata": {},
   "source": [
    "Now, train your GNN using the following script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73cca845",
   "metadata": {
    "tags": [
     "q_code"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Marburg Exercises\\machine-learning-with-graphs\\.venv\\Lib\\site-packages\\torch_geometric\\__init__.py:4: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: Could not load this library: D:\\Marburg Exercises\\machine-learning-with-graphs\\.venv\\Lib\\site-packages\\libpyg.pyd\n",
      "  import torch_geometric.typing\n",
      "d:\\Marburg Exercises\\machine-learning-with-graphs\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Cora()\n",
      "Number of graphs: 1\n",
      "Number of nodes: 2708\n",
      "Number of edges: 10556\n",
      "Number of features per node: 1433\n",
      "Number of classes: 7\n",
      "\n",
      "Using device: cpu\n",
      "\n",
      "Model architecture:\n",
      "  Input: 1433 features per node\n",
      "  Hidden: 16 features (after layer 1)\n",
      "  Output: 7 class scores\n",
      "\n",
      "Graph data:\n",
      "  Nodes (papers): 2708\n",
      "  Edges (citations): 10556\n",
      "  Training nodes: 140\n",
      "  Validation nodes: 500\n",
      "  Test nodes: 1000\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch  0: Train Loss = 1.9520\n",
      "Epoch  1: Train Loss = 1.7393\n",
      "Epoch  2: Train Loss = 1.4500\n",
      "Epoch  3: Train Loss = 1.1511\n",
      "Epoch  4: Train Loss = 0.8671\n",
      "Epoch  5: Train Loss = 0.6194\n",
      "Epoch  6: Train Loss = 0.4216\n",
      "Epoch  7: Train Loss = 0.2758\n",
      "Epoch  8: Train Loss = 0.1759\n",
      "Epoch  9: Train Loss = 0.1110\n",
      "Epoch 10: Train Loss = 0.0696\n",
      "Epoch 11: Train Loss = 0.0436\n",
      "Epoch 12: Train Loss = 0.0275\n",
      "Epoch 13: Train Loss = 0.0177\n",
      "Epoch 14: Train Loss = 0.0117\n",
      "Epoch 15: Train Loss = 0.0080\n",
      "Epoch 16: Train Loss = 0.0056\n",
      "Epoch 17: Train Loss = 0.0041\n",
      "Epoch 18: Train Loss = 0.0030\n",
      "Epoch 19: Train Loss = 0.0023\n",
      "Epoch 20: Train Loss = 0.0018\n",
      "Epoch 21: Train Loss = 0.0015\n",
      "Epoch 22: Train Loss = 0.0012\n",
      "Epoch 23: Train Loss = 0.0010\n",
      "Epoch 24: Train Loss = 0.0009\n",
      "Epoch 25: Train Loss = 0.0008\n",
      "Epoch 26: Train Loss = 0.0007\n",
      "Epoch 27: Train Loss = 0.0006\n",
      "Epoch 28: Train Loss = 0.0006\n",
      "Epoch 29: Train Loss = 0.0005\n",
      "Epoch 30: Train Loss = 0.0005\n",
      "Epoch 31: Train Loss = 0.0005\n",
      "Epoch 32: Train Loss = 0.0005\n",
      "Epoch 33: Train Loss = 0.0005\n",
      "Epoch 34: Train Loss = 0.0004\n",
      "Epoch 35: Train Loss = 0.0005\n",
      "Epoch 36: Train Loss = 0.0005\n",
      "Epoch 37: Train Loss = 0.0005\n",
      "Epoch 38: Train Loss = 0.0005\n",
      "Epoch 39: Train Loss = 0.0005\n",
      "Epoch 40: Train Loss = 0.0005\n",
      "Epoch 41: Train Loss = 0.0005\n",
      "Epoch 42: Train Loss = 0.0006\n",
      "Epoch 43: Train Loss = 0.0006\n",
      "Epoch 44: Train Loss = 0.0006\n",
      "Epoch 45: Train Loss = 0.0007\n",
      "Epoch 46: Train Loss = 0.0007\n",
      "Epoch 47: Train Loss = 0.0008\n",
      "Epoch 48: Train Loss = 0.0008\n",
      "Epoch 49: Train Loss = 0.0009\n",
      "\n",
      "Training complete!\n",
      "\n",
      "Test Accuracy: 0.7530\n",
      "Correct predictions: 753/1000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET: CORA - A CITATION NETWORK\n",
    "# ============================================================================\n",
    "# WHAT IS CORA?\n",
    "# - A dataset of scientific papers (nodes) and citations between them (edges)\n",
    "# - 2,708 papers (nodes)\n",
    "# - 10,556 citation links (edges)\n",
    "# - Each paper has:\n",
    "#   * 1,433 features (bag-of-words representation of the paper)\n",
    "#   * 1 label out of 7 categories (e.g., Neural Networks, Reinforcement Learning)\n",
    "#\n",
    "# TASK: Given paper features and citation network, predict the research area\n",
    "#       of each paper (node classification)\n",
    "#\n",
    "# EXAMPLE: If Paper A cites Papers B and C, and B and C are about \"Neural Networks\",\n",
    "#          then Paper A is likely also about \"Neural Networks\"\n",
    "# ============================================================================\n",
    "\n",
    "# Load the Cora dataset from disk (downloads if not present)\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "# Planetoid is a collection of citation network datasets\n",
    "# Other options: 'CiteSeer', 'PubMed'\n",
    "\n",
    "print(f\"Dataset: {dataset}\")\n",
    "print(f\"Number of graphs: {len(dataset)}\")  # 1 (single graph)\n",
    "print(f\"Number of nodes: {dataset[0].num_nodes}\")  # 2,708 papers\n",
    "print(f\"Number of edges: {dataset[0].num_edges}\")  # 10,556 citations\n",
    "print(f\"Number of features per node: {dataset.num_node_features}\")  # 1,433\n",
    "print(f\"Number of classes: {dataset.num_classes}\")  # 7 research areas\n",
    "print()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SETUP: MODEL, DEVICE, DATA, OPTIMIZER\n",
    "# ============================================================================\n",
    "\n",
    "# Determine if GPU is available (much faster for training)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\\n\")\n",
    "\n",
    "# Initialize the GNN model\n",
    "# Architecture: 1433 input features → 16 hidden → 7 output classes\n",
    "model = SimpleGNN(\n",
    "    in_dim=dataset.num_node_features,  # 1,433 (word features)\n",
    "    hidden_dim=16,                      # 16 (compressed representation)\n",
    "    out_dim=dataset.num_classes         # 7 (one score per class)\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model architecture:\")\n",
    "print(f\"  Input: {dataset.num_node_features} features per node\")\n",
    "print(f\"  Hidden: 16 features (after layer 1)\")\n",
    "print(f\"  Output: {dataset.num_classes} class scores\")\n",
    "print()\n",
    "\n",
    "# Get the graph data\n",
    "# In Cora, there's only one big graph containing all papers\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "# DATA STRUCTURE:\n",
    "# data.x: Node features [2708, 1433] - features for each paper\n",
    "# data.edge_index: Graph structure [2, 10556] - which papers cite which\n",
    "# data.y: True labels [2708] - actual research area for each paper\n",
    "# data.train_mask: Boolean mask [2708] - which nodes to use for training\n",
    "# data.val_mask: Boolean mask [2708] - which nodes to use for validation\n",
    "# data.test_mask: Boolean mask [2708] - which nodes to use for testing\n",
    "\n",
    "print(f\"Graph data:\")\n",
    "print(f\"  Nodes (papers): {data.num_nodes}\")\n",
    "print(f\"  Edges (citations): {data.num_edges}\")\n",
    "print(f\"  Training nodes: {data.train_mask.sum().item()}\")\n",
    "print(f\"  Validation nodes: {data.val_mask.sum().item()}\")\n",
    "print(f\"  Test nodes: {data.test_mask.sum().item()}\")\n",
    "print()\n",
    "\n",
    "# IMPORTANT: Semi-supervised learning setup\n",
    "# We only have labels for a subset of nodes (140 for training)\n",
    "# But the GNN can use the entire graph structure and all node features\n",
    "# This is the power of GNNs: they leverage the graph structure!\n",
    "\n",
    "# Initialize optimizer\n",
    "# Adam: adaptive learning rate optimizer\n",
    "# lr=0.01: learning rate (how big each update step is)\n",
    "# weight_decay=5e-4: L2 regularization to prevent overfitting\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=0.01,           # EXAMPLE: If gradient is 2.0, update = -0.01 * 2.0 = -0.02\n",
    "    weight_decay=5e-4  # Penalizes large weights to keep model simple\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING LOOP: TEACH THE MODEL TO CLASSIFY PAPERS\n",
    "# ============================================================================\n",
    "# TRAINING PROCESS:\n",
    "# 1. Forward pass: Make predictions for all nodes\n",
    "# 2. Compute loss: How wrong are predictions on training nodes?\n",
    "# 3. Backward pass: Calculate gradients (how to improve)\n",
    "# 4. Update weights: Adjust model parameters to reduce loss\n",
    "# ============================================================================\n",
    "\n",
    "train_losses = []  # Store loss values for plotting later\n",
    "\n",
    "# Set model to training mode (enables dropout, batch norm, etc. if present)\n",
    "model.train()\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "# Train for 50 epochs (full passes through the data)\n",
    "for epoch in range(50):\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 1: ZERO GRADIENTS\n",
    "    # ========================================================================\n",
    "    # Clear gradients from previous iteration\n",
    "    # Without this, gradients would accumulate across iterations (wrong!)\n",
    "    optimizer.zero_grad()\n",
    "    # ANALOGY: Erasing the blackboard before solving a new problem\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 2: FORWARD PASS\n",
    "    # ========================================================================\n",
    "    # Get predictions for ALL nodes in the graph\n",
    "    out = model(data)\n",
    "    # out.shape = [2708, 7] - 7 class scores for each of 2708 papers\n",
    "    #\n",
    "    # EXAMPLE OUTPUT for one node:\n",
    "    # out[0] = [0.2, 0.5, 0.1, 0.05, 0.05, 0.05, 0.05]\n",
    "    #          ↑ scores for each of 7 classes\n",
    "    # Higher score = model thinks this class is more likely\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 3: COMPUTE LOSS (only on training nodes!)\n",
    "    # ========================================================================\n",
    "    # Cross-entropy loss: measures how different predictions are from true labels\n",
    "    # We ONLY compute loss on nodes in the training set (data.train_mask)\n",
    "    loss = F.cross_entropy(\n",
    "        out[data.train_mask],    # Predictions for training nodes only [140, 7]\n",
    "        data.y[data.train_mask]  # True labels for training nodes only [140]\n",
    "    )\n",
    "    # EXAMPLE:\n",
    "    # If true label is class 1, and predictions are [0.1, 0.8, 0.05, ...]:\n",
    "    #   Good! High score for correct class → low loss\n",
    "    # If predictions are [0.5, 0.2, 0.1, ...]:\n",
    "    #   Bad! Low score for correct class → high loss\n",
    "    #\n",
    "    # Cross-entropy formula: -log(probability of correct class)\n",
    "    # So we want to maximize the probability of the correct class\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 4: BACKWARD PASS\n",
    "    # ========================================================================\n",
    "    # Compute gradients: how should each weight change to reduce loss?\n",
    "    loss.backward()\n",
    "    # This calculates ∂loss/∂weight for every parameter in the model\n",
    "    # EXAMPLE: If increasing weight W by 0.01 would decrease loss by 0.1,\n",
    "    #          then gradient is -10.0 (negative because loss goes down)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 5: UPDATE WEIGHTS\n",
    "    # ========================================================================\n",
    "    # Adjust model parameters using computed gradients\n",
    "    optimizer.step()\n",
    "    # For each weight: weight_new = weight_old - learning_rate * gradient\n",
    "    # EXAMPLE: weight = 0.5, gradient = -10.0, lr = 0.01\n",
    "    #          weight_new = 0.5 - 0.01 * (-10.0) = 0.5 + 0.1 = 0.6\n",
    "    \n",
    "    # Record loss for this epoch\n",
    "    train_losses.append(loss.item())\n",
    "    print(f\"Epoch {epoch:2d}: Train Loss = {loss.item():.4f}\")\n",
    "    \n",
    "    # WHAT'S HAPPENING EACH EPOCH:\n",
    "    # - Model makes predictions using current weights\n",
    "    # - Compares predictions to true labels (for training nodes only)\n",
    "    # - Adjusts weights to make better predictions\n",
    "    # - Over time, loss should decrease (model is learning!)\n",
    "\n",
    "print(\"\\nTraining complete!\\n\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION: TEST THE MODEL ON UNSEEN DATA\n",
    "# ============================================================================\n",
    "# IMPORTANT: We test on nodes the model has NEVER seen labels for during training\n",
    "# This tells us if the model learned general patterns (good) or just\n",
    "# memorized the training data (bad, called overfitting)\n",
    "# ============================================================================\n",
    "\n",
    "# Set model to evaluation mode (disables dropout, etc.)\n",
    "model.eval()\n",
    "# In eval mode: model behaves deterministically (no randomness)\n",
    "\n",
    "# Get predictions for all nodes\n",
    "# We don't need gradients for evaluation, so we can skip that computation\n",
    "with torch.no_grad():  # Saves memory and speeds up computation\n",
    "    out = model(data)\n",
    "    # out.shape = [2708, 7] - scores for all papers\n",
    "\n",
    "# ========================================================================\n",
    "# CONVERT SCORES TO PREDICTED CLASSES\n",
    "# ========================================================================\n",
    "# For each node, pick the class with the highest score\n",
    "pred = out.argmax(dim=1)\n",
    "# argmax finds the index of the maximum value\n",
    "# EXAMPLE: out[0] = [0.1, 0.8, 0.2, 0.05, 0.01, 0.03, 0.01]\n",
    "#          pred[0] = 1 (index of max value 0.8)\n",
    "# pred.shape = [2708] - one predicted class per paper\n",
    "\n",
    "# ========================================================================\n",
    "# CALCULATE ACCURACY ON TEST SET\n",
    "# ========================================================================\n",
    "# Compare predictions to true labels (only for test nodes)\n",
    "correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "# EXAMPLE: If 800 out of 1000 test predictions are correct:\n",
    "#          correct = 800\n",
    "\n",
    "# Calculate accuracy as a percentage\n",
    "acc = int(correct) / int(data.test_mask.sum())\n",
    "# EXAMPLE: 800 correct / 1000 test nodes = 0.80 = 80% accuracy\n",
    "\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n",
    "print(f\"Correct predictions: {int(correct)}/{int(data.test_mask.sum())}\")\n",
    "print()\n",
    "\n",
    "# ========================================================================\n",
    "# INTERPRETING RESULTS\n",
    "# ========================================================================\n",
    "# WHAT IS GOOD ACCURACY?\n",
    "# - Random guessing: 1/7 ≈ 14.3% (7 classes)\n",
    "# - Good GNN on Cora: 70-85%\n",
    "# - State-of-the-art: 85-90%\n",
    "#\n",
    "# WHY DOES THIS WORK?\n",
    "# The GNN learns that papers citing similar papers are likely in the same\n",
    "# research area. By aggregating information from neighbors (cited papers),\n",
    "# each paper's representation becomes more informative.\n",
    "#\n",
    "# EXAMPLE:\n",
    "# - Paper A (unlabeled) cites Papers B, C, D\n",
    "# - Papers B, C, D are all labeled \"Neural Networks\"\n",
    "# - GNN learns: Paper A is probably also about \"Neural Networks\"\n",
    "# ========================================================================\n",
    "\n",
    "# Optional: Visualize training progress\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Training loss should generally decrease over time\n",
    "# If it increases or plateaus early, might need to:\n",
    "# - Adjust learning rate\n",
    "# - Add more layers\n",
    "# - Increase hidden dimension\n",
    "# - Train for more epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298d6f76",
   "metadata": {},
   "source": [
    "2. Now, modify the `SimpleGNN` class so that it takes the number of layers as input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b33fa74",
   "metadata": {},
   "source": [
    "3. Now, train two GNNs, one with 2 layers and another with 16. Then, compare the Mean Average Distance (MAD) of the embeddings learned by the models using the given function.\n",
    "\n",
    "    - Mean Average Distance (MAD): Average pairwise Euclidean distance between node embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cafd2686",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, num_layers=2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        # dims: [in_dim, hidden_dim, ..., hidden_dim, out_dim]\n",
    "        dims = [in_dim] + [hidden_dim] * (num_layers - 1) + [out_dim]\n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.append(SimpleGNNLayer(dims[i], dims[i + 1]))\n",
    "        \n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x, edge_index)\n",
    "\n",
    "        if i != len(self.layers) - 1:\n",
    "            # last layer -> no ReLU\n",
    "            x = F.relu(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffc5b255",
   "metadata": {
    "tags": [
     "q_code"
    ]
   },
   "outputs": [],
   "source": [
    "def get_mad(embeddings):\n",
    "\n",
    "    # embeddings: [N, d]  (N nodes, d-dimensional embeddings)\n",
    "    N = embeddings.size(0)\n",
    "\n",
    "    # Normalize embeddings\n",
    "    embeddings = F.normalize(embeddings, dim=1)\n",
    "\n",
    "    # Compute all pairwise Euclidean distances → shape [N, N]\n",
    "    dist_matrix = torch.cdist(embeddings, embeddings, p=2)\n",
    "\n",
    "    # Average the distances over all distinct pairs\n",
    "    mad_val = dist_matrix.sum() / (N * (N - 1))\n",
    "\n",
    "    return mad_val.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "344c7f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training GNN with 2 layers:\n",
      "Test Accuracy: 0.7780\n",
      "MAD of final embeddings: 1.3204\n",
      "\n",
      "Training GNN with 32 layers:\n",
      "Test Accuracy: 0.0640\n",
      "MAD of final embeddings: 0.0001\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "# load the Cora dataset\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "\n",
    "# model, data, optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "for L in [2, 32]:\n",
    "    print(f\"\\nTraining GNN with {L} layers:\")\n",
    "    model = SimpleGNN(in_dim=dataset.num_node_features, hidden_dim=16, out_dim=dataset.num_classes, num_layers=L).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    \n",
    "    # training loop\n",
    "    train_losses = []\n",
    "    model.train()\n",
    "    for epoch in range(50):\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "    \n",
    "        # training loss\n",
    "        loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        # print(f\"Epoch {epoch}: Train Loss = {loss.item():.4f}\")\n",
    "\n",
    "    # final eval\n",
    "    model.eval()\n",
    "\n",
    "    pred = model(data).argmax(dim=1)\n",
    "    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "    acc = int(correct) / int(data.test_mask.sum())\n",
    "\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "    print(f\"MAD of final embeddings: {get_mad(model(data)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178ac472",
   "metadata": {},
   "source": [
    "4. Interpret the accuracy and MAD values of models with 2 and 16 layers. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-with-graphs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
