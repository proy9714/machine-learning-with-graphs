{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cd94633",
   "metadata": {},
   "source": [
    "# GNN Augmentation and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3861cc53",
   "metadata": {},
   "source": [
    "## Do We Actually Need Prediction Heads?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589bf92c",
   "metadata": {},
   "source": [
    "Consider a GNN whose last layer produces node embeddings $H^{(L)} \\in \\mathbb{R}^{n \\times d}$. Let's assume that you propose to remove all prediction heads and directly train using:\n",
    "\n",
    "$$\n",
    "    \\mathcal{L} = \\sum_{v \\in \\mathcal{D}} \\ell(h_v^{(L)}, y_v)\n",
    "$$\n",
    "where $\\ell$ is a classification/regression loss and $\\mathcal{D} \\subseteq V$ is the set of labeled nodes used in training.\n",
    "\n",
    "Answer briefly:\n",
    "\n",
    "1. How would this affect the generalization capability of the model? (Hint: Think of what the embeddings will encode.)\n",
    "\n",
    "1. In this scenario, describe what happens when classifying a previously unseen node (inference). What does the embedding of this node represent, and how is the class assigned?\n",
    "\n",
    "1. Explain why adding a prediction head (e.g., a linear layer + softmax) solves the issues identified in (1) and (2). How does it allow embeddings to remain generalizable while still supporting accurate classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d743d9e",
   "metadata": {},
   "source": [
    "## Expressivity of Prediction Heads\n",
    "\n",
    "You are given a GNN that computes node embeddings $h_u$, $h_v$. You think of the following edge prediction heads:\n",
    "\n",
    "- Dot product: $s(u,v)=h_u^\\top h_v$\n",
    "- Single MLP on concatenation: $\\text{MLP}([h_u || h_v])$\n",
    "- Bilinear form: $h_u^\\top W h_v$\n",
    "\n",
    "Tasks:\n",
    "\n",
    "1. Compare the expressive power of these edge prediction heads.\n",
    "\n",
    "1. Show that the dot product is a special (degenerate) case of the bilinear form.\n",
    "\n",
    "1. Give an example edge-labeling function where dot-product performs strictly worse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9069a759",
   "metadata": {},
   "source": [
    "## Is there a Leak?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7c5f07",
   "metadata": {},
   "source": [
    "You train a node classifier on a citation graph, and someone claims:\n",
    "\n",
    "> \"If the graph is fixed, transductive evaluation is just cheating, you always leak test information.\"\n",
    "\n",
    "Tasks:\n",
    "\n",
    "1. Provide a rigorous argument for when this is not true (give a scenario where transductive inference is legitimate).\n",
    "\n",
    "1. Give a leakage scenario that this claim can be true.\n",
    "\n",
    "1. Explain why graph classification cannot be transductive even in principle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff59385",
   "metadata": {},
   "source": [
    "## Programming: Transductive Link Prediction with Multiple Heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620f2006",
   "metadata": {},
   "source": [
    "In this exercise, you'll implement a basic transductive link prediction setup using `PyG` tools on the `Cora` dataset.\n",
    "\n",
    "We start by loading the dataset from the `Planetoid` class ([documentation](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.Planetoid.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7afb5446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "# load dataset\n",
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "data = dataset[0]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31476cc7",
   "metadata": {},
   "source": [
    "Next, we'll use the `RandomLinkSplit` ([documentation](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.transforms.RandomLinkSplit.html)) to split our dataset.\n",
    "\n",
    "For our simple exercise, we won't need a validation set, and we'll split the data so that $90\\%$ of the edges are in the training set and the remaining $10\\%$ is in the test set. Further, we'll split the training edges into message ($80\\%$) and supervision ($20\\%$) edges.\n",
    "\n",
    "1. Given this, complete the following code snippet. Check the `RandomLinkSplit` documentation to find which parameters to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac2dee1",
   "metadata": {
    "tags": [
     "q_code"
    ]
   },
   "outputs": [],
   "source": [
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "# edge split for link prediction\n",
    "split = RandomLinkSplit(\n",
    "    ########## Your code here ##########\n",
    "    \n",
    "    #################################### \n",
    "    is_undirected=True,\n",
    "    add_negative_train_samples=True\n",
    ")\n",
    "train_data, _, test_data = split(data)\n",
    "\n",
    "train_data, test_data = train_data.to(device), test_data.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73f2409",
   "metadata": {},
   "source": [
    "Next, let's be sure that the message and supervision sets do not overlap. We want to have zero overlapping edges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e328ed",
   "metadata": {
    "tags": [
     "q_code"
    ]
   },
   "outputs": [],
   "source": [
    "train_message_edges = train_data.edge_index\n",
    "train_supervision_edges = train_data.edge_label_index\n",
    "test_edges = test_data.edge_index\n",
    "\n",
    "# message edges as set of (u, v) tuples\n",
    "message_set = set(map(tuple, train_message_edges.t().cpu().numpy()))\n",
    "\n",
    "# supervision edges as set of (u, v) tuples\n",
    "supervision_set = set(map(tuple, train_supervision_edges.t().cpu().numpy()))\n",
    "\n",
    "overlap = message_set & supervision_set  # intersection\n",
    "print(f\"Number of overlapping edges: {len(overlap)}\")\n",
    "\n",
    "# number of edges in each set\n",
    "print(f\"Number of message edges: {train_message_edges.size(1)}\")\n",
    "print(f\"Number of supervision edges: {train_supervision_edges.size(1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4016f440",
   "metadata": {},
   "source": [
    "Next, we'll use the `GCNConv` layer implemented in `PyG`. We'll use a simple 2-layer GCN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0541128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=64, out_dim=32):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2940981",
   "metadata": {},
   "source": [
    "Now, we'll need a prediction head. The simplest one would be the dot product.\n",
    "\n",
    "2. Complete the following function that computes the dot products for given edges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296905a3",
   "metadata": {
    "tags": [
     "q_code"
    ]
   },
   "outputs": [],
   "source": [
    "def predict_dot(z, edge_index):\n",
    "    \"\"\"\n",
    "    z: node embeddings, shape [num_nodes, embedding_dim]\n",
    "    edge_index: edge indices, shape [2, num_edges]\n",
    "    returns: scores for each edge, shape [num_edges]\n",
    "    \"\"\"\n",
    "\n",
    "    ########## Your code here ##########\n",
    "    \n",
    "    #################################### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843e76d7",
   "metadata": {},
   "source": [
    "At this point, we have everything we need, so let's train our GCN!\n",
    "\n",
    "3. Complete the following training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5adfab9",
   "metadata": {
    "tags": [
     "q_code"
    ]
   },
   "outputs": [],
   "source": [
    "model = GCN(dataset.num_features).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get node embeddings\n",
    "    ########## Your code here ##########\n",
    "    z = ...\n",
    "    #################################### \n",
    "\n",
    "    # get predictions for supervision edges\n",
    "    ########## Your code here ##########\n",
    "    preds = ...\n",
    "    ####################################\n",
    "    \n",
    "    # labels of supervision edges \n",
    "    labels = train_data.edge_label.float().to(device)\n",
    "\n",
    "    # compute BCE loss\n",
    "    ########## Your code here ##########\n",
    "    loss = F.binary_cross_entropy_with_logits(..., ...)\n",
    "    ####################################\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575acba0",
   "metadata": {},
   "source": [
    "Now it's time to test our model. We'll use the `roc_auc_score` from `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aff548",
   "metadata": {
    "tags": [
     "q_code"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # node embeddings computed using message-passing edges\n",
    "    z = model(data.x.to(device), data.edge_index.to(device))\n",
    "    \n",
    "    # supervision edges for test\n",
    "    test_edge_index = test_data.edge_label_index\n",
    "    test_labels = test_data.edge_label.float().to(device)\n",
    "    \n",
    "    # dot product predictions\n",
    "    test_preds = predict_dot(z, test_edge_index)\n",
    "    \n",
    "    # ROC-AUC\n",
    "    test_auc = roc_auc_score(test_labels.cpu(), test_preds.sigmoid().cpu())\n",
    "    print(f\"Test ROC-AUC: {test_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e846f9d3",
   "metadata": {},
   "source": [
    "4. Instead of dot product, implement the CONCAT + MLP approach and test your GCN. Compare your result with the dot product head, and interpret your findings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
